{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignement3",
      "provenance": [],
      "authorship_tag": "ABX9TyNB5gCx0olj/CNjzmM2ax5J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samibahig/IFT6135/blob/main/Assignement3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xelj_cEIBuWy"
      },
      "source": [
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def log_likelihood_bernoulli(mu, target):\n",
        "\n",
        "    \"\"\" \n",
        "\n",
        "    COMPLETE ME. DONT MODIFY THE PARAMETERS OF THE FUNCTION. Otherwise, tests might fail.\n",
        "\n",
        "\n",
        "\n",
        "    *** note. ***\n",
        "\n",
        "\n",
        "\n",
        "    :param mu: (FloatTensor) - shape: (batch_size x input_size) - The mean of Bernoulli random variables p(x=1).\n",
        "\n",
        "    :param target: (FloatTensor) - shape: (batch_size x input_size) - Target samples (binary values).\n",
        "\n",
        "    :return: (FloatTensor) - shape: (batch_size,) - log-likelihood of target samples on the Bernoulli random variables.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # init\n",
        "\n",
        "    batch_size = mu.size(0)\n",
        "\n",
        "    mu = mu.view(batch_size, -1)\n",
        "\n",
        "    target = target.view(batch_size, -1)\n",
        "\n",
        "\n",
        "\n",
        "    # log_likelihood_bernoulli\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def log_likelihood_normal(mu, logvar, z):\n",
        "\n",
        "    \"\"\" \n",
        "\n",
        "    COMPLETE ME. DONT MODIFY THE PARAMETERS OF THE FUNCTION. Otherwise, tests might fail.\n",
        "\n",
        "\n",
        "\n",
        "    *** note. ***\n",
        "\n",
        "\n",
        "\n",
        "    :param mu: (FloatTensor) - shape: (batch_size x input_size) - The mean of Normal distributions.\n",
        "\n",
        "    :param logvar: (FloatTensor) - shape: (batch_size x input_size) - The log variance of Normal distributions.\n",
        "\n",
        "    :param z: (FloatTensor) - shape: (batch_size x input_size) - Target samples.\n",
        "\n",
        "    :return: (FloatTensor) - shape: (batch_size,) - log probability of the sames on the given Normal distributions.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # init\n",
        "\n",
        "    batch_size = mu.size(0)\n",
        "\n",
        "    mu = mu.view(batch_size, -1)\n",
        "\n",
        "    logvar = logvar.view(batch_size, -1)\n",
        "\n",
        "    z = z.view(batch_size, -1)\n",
        "\n",
        "\n",
        "\n",
        "    # log normal\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def log_mean_exp(y):\n",
        "\n",
        "    \"\"\" \n",
        "\n",
        "    COMPLETE ME. DONT MODIFY THE PARAMETERS OF THE FUNCTION. Otherwise, tests might fail.\n",
        "\n",
        "\n",
        "\n",
        "    *** note. ***\n",
        "\n",
        "\n",
        "\n",
        "    :param y: (FloatTensor) - shape: (batch_size x sample_size) - Values to be evaluated for log_mean_exp. For example log proababilies\n",
        "\n",
        "    :return: (FloatTensor) - shape: (batch_size,) - Output for log_mean_exp.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # init\n",
        "\n",
        "    batch_size = y.size(0)\n",
        "\n",
        "    sample_size = y.size(1)\n",
        "\n",
        "\n",
        "\n",
        "    # log_mean_exp\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def kl_gaussian_gaussian_analytic(mu_q, logvar_q, mu_p, logvar_p):\n",
        "\n",
        "    \"\"\" \n",
        "\n",
        "    COMPLETE ME. DONT MODIFY THE PARAMETERS OF THE FUNCTION. Otherwise, tests might fail.\n",
        "\n",
        "\n",
        "\n",
        "    *** note. ***\n",
        "\n",
        "\n",
        "\n",
        "    :param mu_q: (FloatTensor) - shape: (batch_size x input_size) - The mean of first distributions (Normal distributions).\n",
        "\n",
        "    :param logvar_q: (FloatTensor) - shape: (batch_size x input_size) - The log variance of first distributions (Normal distributions).\n",
        "\n",
        "    :param mu_p: (FloatTensor) - shape: (batch_size x input_size) - The mean of second distributions (Normal distributions).\n",
        "\n",
        "    :param logvar_p: (FloatTensor) - shape: (batch_size x input_size) - The log variance of second distributions (Normal distributions).\n",
        "\n",
        "    :return: (FloatTensor) - shape: (batch_size,) - kl-divergence of KL(q||p).\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # init\n",
        "\n",
        "    batch_size = mu_q.size(0)\n",
        "\n",
        "    mu_q = mu_q.view(batch_size, -1)\n",
        "\n",
        "    logvar_q = logvar_q.view(batch_size, -1)\n",
        "\n",
        "    mu_p = mu_p.view(batch_size, -1)\n",
        "\n",
        "    logvar_p = logvar_p.view(batch_size, -1)\n",
        "\n",
        "\n",
        "\n",
        "    # kld\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def kl_gaussian_gaussian_mc(mu_q, logvar_q, mu_p, logvar_p, num_samples=1):\n",
        "\n",
        "    \"\"\" \n",
        "\n",
        "    COMPLETE ME. DONT MODIFY THE PARAMETERS OF THE FUNCTION. Otherwise, tests might fail.\n",
        "\n",
        "\n",
        "\n",
        "    *** note. ***\n",
        "\n",
        "\n",
        "\n",
        "    :param mu_q: (FloatTensor) - shape: (batch_size x input_size) - The mean of first distributions (Normal distributions).\n",
        "\n",
        "    :param logvar_q: (FloatTensor) - shape: (batch_size x input_size) - The log variance of first distributions (Normal distributions).\n",
        "\n",
        "    :param mu_p: (FloatTensor) - shape: (batch_size x input_size) - The mean of second distributions (Normal distributions).\n",
        "\n",
        "    :param logvar_p: (FloatTensor) - shape: (batch_size x input_size) - The log variance of second distributions (Normal distributions).\n",
        "\n",
        "    :param num_samples: (int) - shape: () - The number of sample for Monte Carlo estimate for KL-divergence\n",
        "\n",
        "    :return: (FloatTensor) - shape: (batch_size,) - kl-divergence of KL(q||p).\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # init\n",
        "\n",
        "    batch_size = mu_q.size(0)\n",
        "\n",
        "    input_size = np.prod(mu_q.size()[1:])\n",
        "\n",
        "    mu_q = mu_q.view(batch_size, -1).unsqueeze(1).expand(batch_size, num_samples, input_size)\n",
        "\n",
        "    logvar_q = logvar_q.view(batch_size, -1).unsqueeze(1).expand(batch_size, num_samples, input_size)\n",
        "\n",
        "    mu_p = mu_p.view(batch_size, -1).unsqueeze(1).expand(batch_size, num_samples, input_size)\n",
        "\n",
        "    logvar_p = logvar_p.view(batch_size, -1).unsqueeze(1).expand(batch_size, num_samples, input_size)\n",
        "\n",
        "\n",
        "\n",
        "    # kld\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}