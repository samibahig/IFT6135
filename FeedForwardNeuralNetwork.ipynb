{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FeedForwardNeuralNetwork",
      "provenance": [],
      "mount_file_id": "1z9vZqFX4uX2wfG7a9T3PDlItPoWPVcPk",
      "authorship_tag": "ABX9TyMTa+u1mhFnuAsBWfHqme7K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samibahig/IFT6135/blob/main/FeedForwardNeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLoZPlcYizs5",
        "outputId": "274a37ae-fb47-4610-ff6b-abd61c2923cf"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class NN(object):\n",
        "    def __init__(self,\n",
        "                 hidden_dims=(512, 256),\n",
        "                 datapath='cifar10.pkl',\n",
        "                 n_classes=10,\n",
        "                 epsilon=1e-6,\n",
        "                 lr=7e-4,\n",
        "                 batch_size=1000,\n",
        "                 seed=None,\n",
        "                 activation=\"relu\",\n",
        "                 init_method=\"glorot\",\n",
        "                 normalization=False\n",
        "                 ):\n",
        "\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.n_hidden = len(hidden_dims)\n",
        "        self.datapath = datapath\n",
        "        self.n_classes = n_classes\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.init_method = init_method\n",
        "        self.seed = seed\n",
        "        self.activation_str = activation\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        self.train_logs = {'train_accuracy': [], 'validation_accuracy': [], 'train_loss': [], 'validation_loss': []}\n",
        "\n",
        "        if datapath is not None:\n",
        "            u = pickle._Unpickler(open(datapath, 'rb'))\n",
        "            u.encoding = 'latin1'\n",
        "            self.train, self.valid, self.test = u.load()\n",
        "            if normalization:\n",
        "                self.normalize()\n",
        "        else:\n",
        "            self.train, self.valid, self.test = None, None, None\n",
        "\n",
        "    def initialize_weights(self, dims):\n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed)\n",
        "\n",
        "        self.weights = {}\n",
        "        # self.weights is a dictionnary with keys W1, b1, W2, b2, ..., Wm, Bm where m - 1 is the number of hidden layers\n",
        "        all_dims = [dims[0]] + list(self.hidden_dims) + [dims[1]]\n",
        "        for layer_n in range(1, self.n_hidden + 2):\n",
        "            # WRITE CODE HERE\n",
        "            self.weights[f\"b{layer_n}\"] = np.zeros((1, all_dims[layer_n]))\n",
        "            d1=(all_dims[layer_n-1])\n",
        "            d2 = all_dims[layer_n]   #will this go out of range for last iteration?  to check.\n",
        "            w_range = np.sqrt (6/(d1+d2))\n",
        "            self.weights[f\"W{layer_n}\"] = np.random.uniform(-w_range, w_range, (d1,d2))\n",
        "\n",
        "    #ref: https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html\n",
        "    #ref: https://stackoverflow.com/questions/32109319/how-to-implement-the-relu-function-in-numpy    \n",
        "    def relu(self, x, grad=False):\n",
        "       if grad:\n",
        "          return 1.0 * (x > 0)\n",
        "       else:\n",
        "          return x * (x > 0)\n",
        "\n",
        "    def sigmoid(self, x, grad=False):\n",
        "\n",
        "        sig = 1 / (1 + np.exp(-x))\n",
        "        if grad:\n",
        "            return (sig *(1-sig))\n",
        "        else:\n",
        "            return sig\n",
        "\n",
        "\n",
        "    def tanh(self, x, grad=False):\n",
        "        z1= np.exp(x)\n",
        "        z2= np.exp(-x)\n",
        "        tanh = (z1-z2)/(z1+z2)\n",
        "\n",
        "        if grad:\n",
        "            return (1 - tanh*tanh)\n",
        "        else:\n",
        "            return tanh\n",
        "\n",
        "    #ref1:  https://stackoverflow.com/questions/48102882/how-to-implement-the-derivative-of-leaky-relu-in-python/48102959\n",
        "    #note that deriv is not defined for x==0, what will happen (will be treated same as false)\n",
        "    def leakyrelu(self, x, grad=False):\n",
        "        alpha = 0.01\n",
        "        if grad:\n",
        "          return (1.0 *(x > 0)) + (alpha* (x <0))\n",
        "        else:\n",
        "          return (x * (x > 0)) + (alpha*x*(x <0))\n",
        "\n",
        "    def activation(self, x, grad=False):\n",
        "        if self.activation_str == \"relu\":\n",
        "            ret=self.relu (x, grad)            \n",
        "        elif self.activation_str == \"sigmoid\":\n",
        "            ret=self.sigmoid (x, grad)            \n",
        "        elif self.activation_str == \"tanh\":\n",
        "            ret=self.tanh (x, grad)\n",
        "        elif self.activation_str == \"leakyrelu\":\n",
        "            ret=self.leakyrelu (x, grad)            \n",
        "        else:\n",
        "            raise Exception(\"invalid\")\n",
        "        return ret\n",
        "\n",
        "\n",
        "\n",
        "    def softmax(self,x):\n",
        "       \n",
        "        z = x - np.max(x, axis=-1, keepdims=True)\n",
        "\n",
        "        numerator = np.exp(z)\n",
        "\n",
        "        denominator = np.sum(numerator, axis=-1, keepdims=True)\n",
        "\n",
        "        ret = numerator / denominator\n",
        "\n",
        "        return ret\n",
        "\n",
        "    #ref: https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python\n",
        "    #axis arg important for 2D array case\n",
        "    def softmax_old(self, x):\n",
        "        # Remember that softmax(x-C) = softmax(x) when C is a constant.\n",
        "        #print (x.shape)\n",
        "        #print (x)\n",
        "\n",
        "        print (\"inside softmax1\", x.ndim)\n",
        "\n",
        "        if (x.ndim ==1):\n",
        "         ax=0\n",
        "        else:\n",
        "         ax=1\n",
        "\n",
        "        ex = np.exp(x-np.amax(x))\n",
        "        print (\"inside softmax2\")\n",
        "        sum= np.sum(ex, axis=ax)\n",
        "        print (\"inside softmax3\")\n",
        "        ret = ex / sum\n",
        "        print (\"inside softmax4\")\n",
        "\n",
        "        #print (ret.shape)\n",
        "        #print (ret)\n",
        "\n",
        "        return ret\n",
        "\n",
        "\n",
        "\n",
        "    def forward_original(self, x):\n",
        "        cache = {\"Z0\": x}\n",
        "        # cache is a dictionnary with keys Z0, A0, ..., Zm, Am where m - 1 is the number of hidden layers\n",
        "        # Ai corresponds to the preactivation at layer i, Zi corresponds to the activation at layer i\n",
        "        # WRITE CODE HERE\n",
        "        pass\n",
        "        return cache\n",
        "\n",
        "    # self.weights is a dictionnary with keys W1, b1, W2, b2, ..., Wm, Bm where m - 1 is the number of hidden layers\n",
        "    # for 2 layered network W1,b1, W2,b2, W3,b3 (3rd layer is output)\n",
        "    def forward(self, x):\n",
        "        cache = {\"Z0\": x}\n",
        "        # cache is a dictionnary with keys Z0, A0, ..., Zm, Am where m - 1 is the number of hidden layers\n",
        "        # Ai corresponds to the preactivation at layer i, Zi corresponds to the activation at layer i\n",
        "\n",
        "        for layer_n in range(0, self.n_hidden+1):\n",
        "            #cache[f\"A{layer_n+1}\"] =  cache[f\"Z{layer_n}\"] * self.weights[f\"W{layer_n+1}\"] + self.weights[f\"b{layer_n+1}\"]\n",
        "            \n",
        "            z=cache[f\"Z{layer_n}\"]\n",
        "            w=self.weights[f\"W{layer_n+1}\"]\n",
        "            ##print (\"**layer, z, w:\", layer_n+1, z.shape, w.shape)\n",
        "\n",
        "            cache[f\"A{layer_n+1}\"] =  np.matmul (cache[f\"Z{layer_n}\"],self.weights[f\"W{layer_n+1}\"]) + self.weights[f\"b{layer_n+1}\"]\n",
        "\n",
        "            if (layer_n == self.n_hidden):  #last layer\n",
        "                cache[f\"Z{layer_n+1}\"] = self.softmax(cache[f\"A{layer_n+1}\"])\n",
        "            \n",
        "            else: #middle layer\n",
        "                cache[f\"Z{layer_n+1}\"] = self.activation (cache[f\"A{layer_n+1}\"])\n",
        "\n",
        "        '''  \n",
        "        #code should be doing the following =>\n",
        "        for layerId in range(1, self.n_hidden+1):\n",
        "            #for 1\n",
        "            A1 = Z0*W1 + b1   # \n",
        "            Z1 = Relu (A1)    # \n",
        "            #for2\n",
        "            A2 = Z1*W2 + b2\n",
        "            Z2 = Relu (A2)\n",
        "            #for output layer\n",
        "            A3 = Z2*W3 + b3\n",
        "            Z3 = Softmax (A3)\n",
        "        '''\n",
        "\n",
        "        return cache\n",
        "\n",
        "\n",
        "    def backward_2(self, cache, labels):\n",
        "\n",
        "        output = cache[f\"Z{self.n_hidden + 1}\"]\n",
        "\n",
        "        grads = {}\n",
        "                                                                                        \n",
        "\n",
        "        grads[\"dA\"+str(self.n_hidden + 1)] = output - labels\n",
        "\n",
        "               \n",
        "        grads[\"dZ\"+str(self.n_hidden)] = np.dot(grads[\"dA\"+str(self.n_hidden + 1)], self.weights[\"W\"+str(self.n_hidden + 1)].T)  \n",
        "                \n",
        "        grads[\"dW\"+str(self.n_hidden + 1)] = (1/self.batch_size)*np.dot(cache[\"Z\"+str(self.n_hidden)].T, grads[\"dA\"+str(self.n_hidden + 1)])   \n",
        "\n",
        "        grads[\"db\"+str(self.n_hidden + 1)] = (1/self.batch_size)*np.sum(grads[\"dA\"+str(self.n_hidden + 1)], axis=0, keepdims=True)\n",
        "          \n",
        "\n",
        "        for layer_n in reversed(range(1,self.n_hidden +1)):\n",
        "\n",
        "            grads[\"dA\"+str(layer_n)] = grads[\"dZ\"+str(layer_n)]*self.activation(cache[\"Z\"+str(layer_n)],True)\n",
        "\n",
        "            if (layer_n>1):\n",
        "\n",
        "                grads[\"dZ\"+str(layer_n-1)] = np.dot(grads[\"dA\"+str(layer_n)],self.weights[\"W\"+str(layer_n)].T)\n",
        "\n",
        "            grads[\"dW\"+str(layer_n)] = (1/self.batch_size)*np.dot(cache[\"Z\"+str(layer_n)].T, grads[\"dA\"+str(layer_n)])   \n",
        "\n",
        "            grads[\"db\"+str(layer_n)] = (1/self.batch_size)*np.sum(grads[\"dA\"+str(layer_n)], axis = 0,keepdims=True)\n",
        "\n",
        "        #print(grads)      \n",
        "\n",
        "        return grads\n",
        "\n",
        "    '''\n",
        "       output = m x n\n",
        "       labels = batch_size x m\n",
        "\n",
        "       Backward should have following entries\n",
        "       dA3, dW3, db3\n",
        "       dZ2, dA2, dW2, db2\n",
        "       dZ1, dA1, dW1, db1\n",
        "    '''\n",
        "\n",
        "    def backward (self, cache, labels):\n",
        "        output = cache[f\"Z{self.n_hidden + 1}\"]\n",
        "        grads = {}\n",
        "        # grads is a dictionnary with keys dAm, dWm, dbm, dZ(m-1), dA(m-1), ..., dW1, db1\n",
        "        # WRITE CODE HERE\n",
        "\n",
        "        #print (\"self.batchsize=\", self.batch_size)\n",
        "        #print (\"labels.shape=\", labels.shape)\n",
        "\n",
        "        #1) Evaluate dA3,  f(x) = output of the NN, = Z3\n",
        "        grads[f\"dA{self.n_hidden+1}\"] =  output - labels\n",
        "\n",
        "        LL = self.n_hidden + 1\n",
        "\n",
        "        for kk in range (LL,0,-1):    # do this for LL=3,2,1\n",
        "\n",
        "            #todo: Check shapes in below equation\n",
        "            zz = cache[f\"Z{kk-1}\"]\n",
        "            zz_t = np.transpose (zz)\n",
        "            #print (\"kk,shape1,shape2\", kk, zz_t.shape, grads[f\"dA{kk}\"].shape )\n",
        "\n",
        "            grads[f\"dW{kk}\"] = (1/self.batch_size)*(np.matmul(  zz_t, grads[f\"dA{kk}\"]))\n",
        "            #grads[f\"db{kk}\"] = (1/self.batch_size)*grads[f\"dA{kk}\"]\n",
        "\n",
        "            #print (\"kk,dA:\", kk, grads[f\"dA{kk}\"])\n",
        "\n",
        "            grads[f\"db{kk}\"] = (1/self.batch_size)*np.sum(grads[f\"dA{kk}\"], axis=0, keepdims=True)  #todo- check the args\n",
        "            \n",
        "            #print (\"kk,dB:\", kk, grads[f\"db{kk}\"])\n",
        "\n",
        "            if (kk > 1):              #do this only for LL=3,2\n",
        "                \n",
        "                #todo: Check shapes in below equation\n",
        "                ww = self.weights[f\"W{kk}\"]\n",
        "                ww_t = np.transpose (ww)\n",
        "\n",
        "                #print (\"kk,shape1a,shape2a\", kk, grads[f\"dA{kk}\"].shape, ww_t.shape )\n",
        "                grads[f\"dZ{kk-1}\"] = np.matmul(  grads[f\"dA{kk}\"], ww_t )\n",
        "\n",
        "\n",
        "                #todo: Check shapes in below equation\n",
        "                deriv  = self.activation( cache[f\"Z{kk-1}\"], grad=True) ## TODO - Find this term\n",
        "                deriv_tr = np.transpose (deriv)\n",
        "                \n",
        "                #print (\"kk, g.shape, d.shape\", kk, grads[f\"dZ{kk-1}\"].shape, deriv.shape)\n",
        "                \n",
        "                grads[f\"dA{kk-1}\"] = grads[f\"dZ{kk-1}\"] * (deriv)\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def update(self, grads):\n",
        "        for layer in range(1, self.n_hidden + 2):\n",
        "            # WRITE CODE HERE\n",
        "            self.weights[f\"b{layer}\"] -= grads[f\"db{layer}\"]*self.lr\n",
        "            self.weights[f\"W{layer}\"] -= grads[f\"dW{layer}\"]*self.lr\n",
        "            \n",
        "\n",
        "    #TODO: Test this function\n",
        "    def one_hot(self, y):\n",
        "        # WRITE CODE HERE\n",
        "        \n",
        "        batch_size = len(y)\n",
        "        num = np.amax(y)\n",
        "        out = np.zeros ((batch_size, num+1),dtype=int)\n",
        "\n",
        "        for i,yy in enumerate (y):\n",
        "            out[i][yy]=1\n",
        "        \n",
        "        return out\n",
        "\n",
        "    # predictions = batch_size x n_classes\n",
        "    # labels = batch_size x n_classes\n",
        "    # Ref: https://pmirla.github.io/2016/10/09/Basics-crossEntropy.html\n",
        "    #cross-entripy loss formula\n",
        "    #1/n sigma (-np.log(qi))  => this is because labels is 1 hot (need to multiply only true label with its actual prob as reported by softmax)\n",
        "    def loss(self, prediction, labels):\n",
        "        prediction[np.where(prediction < self.epsilon)] = self.epsilon\n",
        "        prediction[np.where(prediction > 1 - self.epsilon)] = 1 - self.epsilon\n",
        "        \n",
        "        # WRITE CODE HERE\n",
        "        #print (\"p.shape, labels.shape:\",prediction.shape, labels.shape )\n",
        "        batch_size = labels.shape[0]\n",
        "        n_classes = labels.shape[1]\n",
        "\n",
        "        label_index = np.argmax(labels, axis=1)\n",
        "\n",
        "        #print (\"len(label_index):\",len(label_index))\n",
        "        assert (batch_size == len(label_index))\n",
        "        \n",
        "        logVal=0.0\n",
        "        for i,index in enumerate (label_index):\n",
        "            #print (\"i,index, val\", i, index, prediction[i][index])\n",
        "            logVal += -np.log (prediction[i][index])\n",
        "\n",
        "        \n",
        "        logVal = logVal/batch_size\n",
        "\n",
        "        \n",
        "        return logVal\n",
        "\n",
        "    def compute_loss_and_accuracy(self, X, y):\n",
        "        one_y = self.one_hot(y)\n",
        "        cache = self.forward(X)\n",
        "        predictions = np.argmax(cache[f\"Z{self.n_hidden + 1}\"], axis=1)\n",
        "        accuracy = np.mean(y == predictions)\n",
        "        loss = self.loss(cache[f\"Z{self.n_hidden + 1}\"], one_y)\n",
        "        return loss, accuracy, predictions\n",
        "\n",
        "    def train_loop(self, n_epochs):\n",
        "        X_train, y_train = self.train\n",
        "        y_onehot = self.one_hot(y_train)\n",
        "        dims = [X_train.shape[1], y_onehot.shape[1]]\n",
        "        self.initialize_weights(dims)\n",
        "\n",
        "        n_batches = int(np.ceil(X_train.shape[0] / self.batch_size))\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            \n",
        "            #print (\"Training epoch, n_batches:\", epoch+1, n_batches)\n",
        "\n",
        "            for batch in range(n_batches):\n",
        "                minibatchX = X_train[self.batch_size * batch:self.batch_size * (batch + 1), :]\n",
        "                minibatchY = y_onehot[self.batch_size * batch:self.batch_size * (batch + 1), :]\n",
        "                # WRITE CODE HERE\n",
        "                \n",
        "                cache = self.forward(minibatchX)\n",
        "                grads = self.backward(cache, minibatchY)\n",
        "                self.update(grads)\n",
        "\n",
        "            X_train, y_train = self.train\n",
        "            train_loss, train_accuracy, _ = self.compute_loss_and_accuracy(X_train, y_train)\n",
        "            X_valid, y_valid = self.valid\n",
        "            valid_loss, valid_accuracy, _ = self.compute_loss_and_accuracy(X_valid, y_valid)\n",
        "\n",
        "            self.train_logs['train_accuracy'].append(train_accuracy)\n",
        "            self.train_logs['validation_accuracy'].append(valid_accuracy)\n",
        "            self.train_logs['train_loss'].append(train_loss)\n",
        "            self.train_logs['validation_loss'].append(valid_loss)\n",
        "\n",
        "            #print (\"Epoch, Acc(Tr/Val), Loss(Tr/Val): %d\\t%f\\t%f\\t%f\\t%f\" % (epoch+1), train_accuracy, valid_accuracy, train_loss, valid_loss)\n",
        "            print(f\"Epoch {epoch+1} | Train loss {train_loss:.04f} | Train acc {train_accuracy:.04f} |\"\n",
        "                      f\" Valid loss {valid_loss:.04f} | Valid acc {valid_accuracy:.04f}\")\n",
        "\n",
        "        return self.train_logs\n",
        "\n",
        "    def evaluate(self):\n",
        "        X_test, y_test = self.test\n",
        "        # WRITE CODE HERE\n",
        "        test_loss, test_accuracy, _ = self.compute_loss_and_accuracy(X_test, y_test)\n",
        "        return (test_loss, test_accuracy)\n",
        "\n",
        "    '''\n",
        "    badguy = list(badguy)\n",
        "    badguy[0]-=7\n",
        "    badguy = tuple(badguy)\n",
        "    '''\n",
        "    def normalize(self):\n",
        "        # WRITE CODE HERE\n",
        "        # compute mean and std along the first axis\n",
        "\n",
        "        mean = np.mean (self.train[0], axis=0)\n",
        "        std = np.std (self.train[0], axis=0)\n",
        "\n",
        "        #print (f\"mean{mean:.04f}, std={std:.04f}\")\n",
        "\n",
        "        '''\n",
        "        self.train[0] = (self.train[0]-mean)/(std)\n",
        "        self.valid[0] = (self.valid[0]-mean)/(std)\n",
        "        self.test[0] = (self.test[0]-mean)/(std)\n",
        "        '''\n",
        "        x_train = (self.train[0]-mean)/(std)\n",
        "        y_train = self.train[1]\n",
        "        x_valid = (self.valid[0]-mean)/(std)\n",
        "        y_valid = self.valid[1]\n",
        "        x_test = (self.test[0]-mean)/(std)\n",
        "        y_test = self.test[1]\n",
        "\n",
        "        self.train = (x_train, y_train)\n",
        "        self.valid = (x_valid, y_valid)\n",
        "        self.test =  (x_test, y_test)\n",
        "\n",
        "        pass\n",
        "\n",
        "\n",
        "def testrelu(x, grad=False):\n",
        "    if grad:\n",
        "       return 1.0 * (x > 0)\n",
        "    else:\n",
        "       return x * (x > 0)\n",
        "\n",
        "'''\n",
        "Trying forward prop using a neural network of hidden dimensions (101, 102, 300) and seed 5692 and relu activation on a subset of svhn\n",
        "Test Failed: The values inside your cache dictionary are wrong\n",
        "'''\n",
        "def test_one_hot( y):\n",
        "        # WRITE CODE HERE\n",
        "        \n",
        "        batch_size = len(y)\n",
        "        num = np.amax(y)\n",
        "        out = np.zeros ((batch_size, num+1),dtype=int)\n",
        "\n",
        "        for i,yy in enumerate (y):\n",
        "            out[i][yy]=1\n",
        "        \n",
        "        return out\n",
        "\n",
        "def SVHN_Q4_depth_and_width ():\n",
        "\n",
        "   #hidden_dims=(512, 120, 120, 120, 120, 120, 120)\n",
        "    \n",
        "   nn1 = NN (datapath='/content/drive/MyDrive/svhn (2) (1).pkl',hidden_dims=(512, 120, 120, 120, 120, 120, 120), lr=0.03, batch_size=100, seed=0, normalization=False)\n",
        "\n",
        "   no_of_epoch = 30\n",
        "   logs = nn1.train_loop (no_of_epoch)\n",
        "\n",
        "   print (\"Finished with training\")\n",
        "   print (logs)\n",
        "\n",
        "\n",
        "\n",
        "def SVHN_Q3_with_normalization ():\n",
        "\n",
        "   '''\n",
        "                    lr=7e-4,\n",
        "                 batch_size=1000,\n",
        "                 seed=None,\n",
        "   '''\n",
        "   nn1 = NN(datapath='svhn.pkl',lr=0.03, batch_size=100, seed=0, normalization=True)\n",
        "\n",
        "   no_of_epoch = 30\n",
        "   logs = nn1.train_loop (no_of_epoch)\n",
        "\n",
        "   print (\"Finished with training\")\n",
        "   print (logs)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #n1 = NN(datapath=None)\n",
        "    \n",
        "    \n",
        "    '''\n",
        "    //check self.valid[0] = 3000x3072 matrix, self.valid[1] = array of 3000 labels\n",
        "    //self.train 67000\n",
        "    //self.valid 3000\n",
        "    //self.test 20000 \n",
        "    //report evolution of training and validation accuracies\n",
        "    //report evolution of training and val loss\n",
        "    //check that at epoch=30, training accuracy > 0.7\n",
        "    '''\n",
        "    #SVHN_Q3_with_normalization()\n",
        "\n",
        "    '''\n",
        "\n",
        "    '''\n",
        "    SVHN_Q4_depth_and_width()\n",
        "\n",
        "   \n",
        "    print (\"Hello!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train loss 2.1234 | Train acc 0.2588 | Valid loss 2.1216 | Valid acc 0.2620\n",
            "Epoch 2 | Train loss 1.7353 | Train acc 0.4051 | Valid loss 1.7280 | Valid acc 0.4047\n",
            "Epoch 3 | Train loss 1.6373 | Train acc 0.4394 | Valid loss 1.6535 | Valid acc 0.4297\n",
            "Epoch 4 | Train loss 1.2176 | Train acc 0.6182 | Valid loss 1.2191 | Valid acc 0.6140\n",
            "Epoch 5 | Train loss 1.3433 | Train acc 0.5664 | Valid loss 1.3415 | Valid acc 0.5730\n",
            "Epoch 6 | Train loss 1.1031 | Train acc 0.6485 | Valid loss 1.1308 | Valid acc 0.6420\n",
            "Epoch 7 | Train loss 1.2465 | Train acc 0.5993 | Valid loss 1.2805 | Valid acc 0.5927\n",
            "Epoch 8 | Train loss 0.9242 | Train acc 0.7058 | Valid loss 0.9663 | Valid acc 0.6930\n",
            "Epoch 9 | Train loss 1.1294 | Train acc 0.6375 | Valid loss 1.1727 | Valid acc 0.6287\n",
            "Epoch 10 | Train loss 1.1167 | Train acc 0.6397 | Valid loss 1.1693 | Valid acc 0.6217\n",
            "Epoch 11 | Train loss 0.9704 | Train acc 0.6905 | Valid loss 1.0277 | Valid acc 0.6747\n",
            "Epoch 12 | Train loss 0.9379 | Train acc 0.6983 | Valid loss 0.9999 | Valid acc 0.6810\n",
            "Epoch 13 | Train loss 0.9182 | Train acc 0.7016 | Valid loss 0.9817 | Valid acc 0.6840\n",
            "Epoch 14 | Train loss 0.9182 | Train acc 0.7019 | Valid loss 0.9831 | Valid acc 0.6830\n",
            "Epoch 15 | Train loss 0.9119 | Train acc 0.7108 | Valid loss 0.9971 | Valid acc 0.6947\n",
            "Epoch 16 | Train loss 0.9699 | Train acc 0.6875 | Valid loss 1.0387 | Valid acc 0.6743\n",
            "Epoch 17 | Train loss 0.7656 | Train acc 0.7539 | Valid loss 0.8371 | Valid acc 0.7400\n",
            "Epoch 18 | Train loss 0.8588 | Train acc 0.7345 | Valid loss 0.9450 | Valid acc 0.7137\n",
            "Epoch 19 | Train loss 0.7259 | Train acc 0.7714 | Valid loss 0.8167 | Valid acc 0.7447\n",
            "Epoch 20 | Train loss 0.7027 | Train acc 0.7779 | Valid loss 0.7992 | Valid acc 0.7583\n",
            "Epoch 21 | Train loss 0.7281 | Train acc 0.7667 | Valid loss 0.8276 | Valid acc 0.7407\n",
            "Epoch 22 | Train loss 0.6790 | Train acc 0.7837 | Valid loss 0.7970 | Valid acc 0.7527\n",
            "Epoch 23 | Train loss 0.7227 | Train acc 0.7760 | Valid loss 0.8530 | Valid acc 0.7533\n",
            "Epoch 24 | Train loss 0.6301 | Train acc 0.7994 | Valid loss 0.7491 | Valid acc 0.7700\n",
            "Epoch 25 | Train loss 0.6352 | Train acc 0.7961 | Valid loss 0.7501 | Valid acc 0.7683\n",
            "Epoch 26 | Train loss 0.6114 | Train acc 0.8041 | Valid loss 0.7465 | Valid acc 0.7777\n",
            "Epoch 27 | Train loss 0.5614 | Train acc 0.8204 | Valid loss 0.7041 | Valid acc 0.7830\n",
            "Epoch 28 | Train loss 0.6274 | Train acc 0.7994 | Valid loss 0.7626 | Valid acc 0.7670\n",
            "Epoch 29 | Train loss 0.6076 | Train acc 0.8046 | Valid loss 0.7501 | Valid acc 0.7647\n",
            "Epoch 30 | Train loss 0.5872 | Train acc 0.8161 | Valid loss 0.7427 | Valid acc 0.7860\n",
            "Finished with training\n",
            "{'train_accuracy': [0.2588358208955224, 0.4051492537313433, 0.4393582089552239, 0.6181791044776119, 0.5663731343283582, 0.6485373134328358, 0.5993283582089552, 0.7057611940298507, 0.6374626865671642, 0.639731343283582, 0.6905373134328359, 0.6982835820895522, 0.7016119402985075, 0.7018955223880597, 0.7107761194029851, 0.6875223880597015, 0.7538805970149254, 0.7344626865671642, 0.7713731343283582, 0.777865671641791, 0.7667462686567165, 0.7836567164179105, 0.7759701492537313, 0.7994029850746268, 0.796089552238806, 0.8041194029850747, 0.8203731343283582, 0.7994328358208955, 0.8046268656716418, 0.816089552238806], 'validation_accuracy': [0.262, 0.4046666666666667, 0.42966666666666664, 0.614, 0.573, 0.642, 0.5926666666666667, 0.693, 0.6286666666666667, 0.6216666666666667, 0.6746666666666666, 0.681, 0.684, 0.683, 0.6946666666666667, 0.6743333333333333, 0.74, 0.7136666666666667, 0.7446666666666667, 0.7583333333333333, 0.7406666666666667, 0.7526666666666667, 0.7533333333333333, 0.77, 0.7683333333333333, 0.7776666666666666, 0.783, 0.767, 0.7646666666666667, 0.786], 'train_loss': [2.1233604773880668, 1.7352849626977085, 1.6373335794971398, 1.2175574380896248, 1.34328902823248, 1.103145531442878, 1.246492124397477, 0.9241905157073935, 1.1293744216660482, 1.1166950126171076, 0.9704291660817683, 0.937899674074369, 0.9181860738268175, 0.9182409795636876, 0.911903534460935, 0.9698895686343256, 0.7655696251714409, 0.8587871906181764, 0.7258856005804669, 0.702650213239482, 0.7281036534226105, 0.679040242208896, 0.722664749783942, 0.6300615980767846, 0.6351923576107303, 0.6114461721248557, 0.5614388732361395, 0.6274378098238504, 0.607559218869628, 0.5872345292936023], 'validation_loss': [2.121630668382795, 1.7279777628640238, 1.653457868832738, 1.219106189254475, 1.341549853145288, 1.1308115679756354, 1.2805414083587132, 0.9663097424336645, 1.1726882060027155, 1.1693281500006012, 1.027694606836311, 0.9998780863288741, 0.981709509217692, 0.9831245548929237, 0.9971480317249598, 1.038699492944718, 0.8370543150009643, 0.9450430385512476, 0.8167118064627531, 0.7991865923443688, 0.8276079541913778, 0.7969537830813442, 0.8529586818105871, 0.7491485754094273, 0.7500800745306309, 0.7464971499953856, 0.7040808609647382, 0.7625611625749544, 0.750079797988154, 0.7426506484485035]}\n",
            "Hello!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3X9ZBfji9WG",
        "outputId": "3a783699-9671-4d02-c2d5-87abc6b0e9f4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "!cp /content/drive/MyDrive/svhn (2) (1).pkl\n",
        "#https://drive.google.com/file/d/1oaKLzGnU3F9BOD-13422T__E9QaDtLEh/view?usp=sharing\n",
        "#https://drive.google.com/file/d/1dTVDhHyi3nIqaOgJ_5tmnypzHPEi_WRC/view?usp=sharing.pkl\n",
        "#!cp /content/drive/MyDrive/6390/Data/test.npz /content/test.npz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "/bin/bash: -c: line 0: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 0: `cp /content/drive/MyDrive/svhn (2) (1).pkl'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "5CaCvfC-0XTw",
        "outputId": "6e5ae0bd-ccb5-4e2c-e50d-eede0ba745c7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "new_arr = []\n",
        "for i in range(30):\n",
        "    new_arr.append(i)\n",
        "plt.plot(new_arr, train_accuracy, label=\"training accuracy\")\n",
        "plt.plot(new_arr, valid_accuracy, label=\"testing accuracy\")\n",
        "plt.title(\"training and validation accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-8a2786e0348e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnew_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"testing accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training and validation accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_accuracy' is not defined"
          ]
        }
      ]
    }
  ]
}