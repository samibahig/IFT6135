{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN",
      "provenance": [],
      "authorship_tag": "ABX9TyPDz2uYdAl3MFtc5q60rKjS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samibahig/IFT6135/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf6Hzilbjhfx"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms\n",
        "\n",
        "\n",
        "from_numpy = torch.from_numpy\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "store_every = 700\n",
        "lr0 = 0.03\n",
        "#model_type = 'MLP'\n",
        "model_type = 'CNN'\n",
        "mnist_transforms = torchvision.transforms.Compose(\n",
        "        [torchvision.transforms.ToTensor()])\n",
        "mnist_train = torchvision.datasets.MNIST(\n",
        "        root='./data', train=True, \n",
        "        transform=mnist_transforms, download=True)\n",
        "mnist_test = torchvision.datasets.MNIST(\n",
        "        root='./data', train=False, \n",
        "        transform=mnist_transforms, download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        mnist_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        mnist_test, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "class ResLinear(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features, activation=nn.ReLU()):\n",
        "        super(ResLinear, self).__init__()\n",
        "        \n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.activation = activation\n",
        "        \n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "        if in_features != out_features:\n",
        "            self.project_linear = nn.Linear(in_features, out_features)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        inner = self.activation(self.linear(x))\n",
        "        if self.in_features != self.out_features:\n",
        "            skip = self.project_linear(x)\n",
        "        else:\n",
        "            skip = x\n",
        "        return inner + skip\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "if model_type == 'CNN':\n",
        "  model = nn.Sequential(nn.Conv2d(1,100,3),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.Conv2d(100,122,5),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        Flatten(),\n",
        "        ResLinear(1952, 100),\n",
        "        nn.ReLU(),\n",
        "        ResLinear(100, 10)\n",
        "    )\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr0)\n",
        "\n",
        "#Learning rate\n",
        "def adjust_lr(optimizer, epoch, total_epochs):\n",
        "    lr = lr0 * (0.1 ** (epoch / float(total_epochs)))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "#Evalution Functions\n",
        "def accuracy(proba, y):\n",
        "    correct = torch.eq(proba.max(1)[1], y).sum().type(torch.FloatTensor)\n",
        "    return correct / y.size(0)\n",
        "    \n",
        "    \n",
        "def evaluate(dataset_loader, criterion):\n",
        "    LOSSES = 0\n",
        "    COUNTER = 0\n",
        "    for batch in dataset_loader:\n",
        "        optimizer.zero_grad()\n",
        "        x,y = batch\n",
        "        if model_type == 'CNN':\n",
        "            x = x.view(-1,1,28,28)\n",
        "            y = y.view(-1)\n",
        "          \n",
        "        loss = criterion(model(x), y)\n",
        "        n = y.size(0)\n",
        "        LOSSES += loss.sum().data.cpu().numpy() * n\n",
        "        COUNTER += n\n",
        "    return LOSSES / float(COUNTER)\n",
        "\n",
        "#Training loop\n",
        "LOSSES = 0\n",
        "COUNTER = 0\n",
        "ITERATIONS = 0\n",
        "train_losses = [] # save loss\n",
        "train_accuracy =[] #save accuracy\n",
        "tess_accuracy =[] #save accuracy\n",
        "test_losses =[] #save accuracy\n",
        "\n",
        "learning_curve_nll_train = list()\n",
        "learning_curve_nll_test = list()\n",
        "learning_curve_acc_train = list()\n",
        "learning_curve_acc_test = list()\n",
        "for e in range(num_epochs):\n",
        "    print('Epoch', e+1)\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        x,y = batch\n",
        "        if model_type == 'CNN':\n",
        "            x = x.view(-1,1,28,28)\n",
        "            y = y.view(-1)\n",
        "            \n",
        "        loss = criterion(model(x), y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        n = y.size(0)\n",
        "        LOSSES += loss.sum().data.cpu().numpy() * n\n",
        "        COUNTER += n\n",
        "        ITERATIONS += 1\n",
        "        if ITERATIONS%(store_every/5) == 0:\n",
        "            avg_loss = LOSSES / float(COUNTER)\n",
        "            LOSSES = 0\n",
        "            COUNTER = 0\n",
        "            print(\" Iteration {}: TRAIN {}\".format(\n",
        "                ITERATIONS, avg_loss))\n",
        "    \n",
        "        if ITERATIONS%(store_every) == 0 or ITERATIONS%(938) == 0 :     \n",
        "            train_loss = evaluate(train_loader, criterion)\n",
        "            learning_curve_nll_train.append(train_loss)\n",
        "            test_loss = evaluate(test_loader, criterion)\n",
        "            learning_curve_nll_test.append(test_loss)\n",
        "\n",
        "            train_acc = evaluate(train_loader, accuracy)\n",
        "            learning_curve_acc_train.append(train_acc)\n",
        "            test_acc = evaluate(test_loader, accuracy)\n",
        "            learning_curve_acc_test.append(test_acc)\n",
        "                    \n",
        "            print(\" [NLL] TRAIN {} / TEST {}\".format(\n",
        "                train_loss, test_loss))\n",
        "            print(\" [ACC] TRAIN {} / TEST {}\".format(\n",
        "                train_acc, test_acc))\n",
        "    \n",
        "    train_accuracy.append(train_acc)\n",
        "    train_losses.append(train_loss)\n",
        "    tess_accuracy.append(test_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    print('Train accuracy', train_accuracy)\n",
        "    print('Train Loss', train_loss)\n",
        "    print('Train test accurary', test_loss)\n",
        "    print\n",
        "    print('Train accuracy', test_acc)\n",
        "    print(ITERATIONS)\n",
        "\n",
        "    \n",
        "    adjust_lr(optimizer, e+1, num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}