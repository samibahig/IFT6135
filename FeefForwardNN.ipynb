{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FeefForwardNN",
      "provenance": [],
      "authorship_tag": "ABX9TyPMeG5LFVoTS0g7HwpDZ6zO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samibahig/IFT6135/blob/main/FeefForwardNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd_g9PzSEW_x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "6445a7a0-f397-47a7-b326-ae29ec7ee35d"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import gzip\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "def one_hot(y, n_classes=10):\n",
        "    return np.eye(n_classes)[y]\n",
        "\n",
        "def load_mnist():\n",
        "    data_file = gzip.open(\"mnist.pkl.gz\", \"rb\")\n",
        "    train_data, val_data, test_data = pickle.load(data_file, encoding=\"latin1\")\n",
        "    data_file.close()\n",
        "\n",
        "    train_inputs = [np.reshape(x, (784, 1)) for x in train_data[0]]\n",
        "    train_results = [one_hot(y, 10) for y in train_data[1]]\n",
        "    train_data = np.array(train_inputs).reshape(-1, 784), np.array(train_results).reshape(-1, 10)\n",
        "\n",
        "    val_inputs = [np.reshape(x, (784, 1)) for x in val_data[0]]\n",
        "    val_results = [one_hot(y, 10) for y in val_data[1]]\n",
        "    val_data = np.array(val_inputs).reshape(-1, 784), np.array(val_results).reshape(-1, 10)\n",
        "\n",
        "    test_inputs = [np.reshape(x, (784, 1)) for x in test_data[0]]\n",
        "    test_data = list(zip(test_inputs, test_data[1]))\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "# train_data_, val_data_, test_data_ = load_mnist()\n",
        "\n",
        "class NN(object):\n",
        "    def __init__(self,\n",
        "                 hidden_dims=(784, 256),\n",
        "                 epsilon=1e-6,\n",
        "                 lr=7e-4,\n",
        "                 batch_size=64,\n",
        "                 seed=None,\n",
        "                 activation=\"relu\",\n",
        "                 data=None,\n",
        "                 init_method =\"glorot\"\n",
        "                 ):\n",
        "\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.n_hidden = len(hidden_dims)\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.init_method = init_method\n",
        "        self.seed = seed\n",
        "        self.activation_str = activation\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        self.train_logs = {'train_accuracy': [], 'validation_accuracy': [], 'train_loss': [], 'validation_loss': []}\n",
        "\n",
        "        if data is None:\n",
        "            # for testing, do NOT remove or modify\n",
        "            self.train, self.valid, self.test = (\n",
        "                (np.random.rand(400, 784), one_hot(np.random.randint(0, 10, 400))),\n",
        "                (np.random.rand(400, 784), one_hot(np.random.randint(0, 10, 400))),\n",
        "                (np.random.rand(400, 784), one_hot(np.random.randint(0, 10, 400)))\n",
        "        )\n",
        "        else:\n",
        "            self.train, self.valid, self.test = data\n",
        "\n",
        "\n",
        "    def initialize_weights(self, dims):        \n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed)\n",
        "\n",
        "        self.weights = {}\n",
        "        # self.weights is a dictionnary with keys W1, b1, W2, b2, ..., Wm, Bm where m - 1 is the number of hidden layers\n",
        "        all_dims = [dims[0]] + list(self.hidden_dims) + [dims[1]]\n",
        "        for layer_n in range(1, self.n_hidden + 2):\n",
        "            # WRITE CODE HERE\n",
        "            d = np.sqrt(1/(all_dims[layer_n-1]+all_dims[layer_n]))\n",
        "            W = np.random.uniform(-d, d, (all_dims[layer_n - 1], all_dims[layer_n]))\n",
        "            self.weights[f\"W{layer_n}\"] = W\n",
        "            self.weights[f\"b{layer_n}\"] = np.zeros((1, all_dims[layer_n]))\n",
        "\n",
        "    def relu(self, x, grad=False):\n",
        "        if grad:\n",
        "            return x > 0 \n",
        "            pass\n",
        "        return np.maximum(x, 0)\n",
        "        pass\n",
        "        return 0\n",
        "\n",
        "    def sigmoid(self, x, grad=False):\n",
        "        if grad:\n",
        "            s = 1/(1 + np.exp(-x))\n",
        "            return s* (s - 1)\n",
        "        return 1/(1 + np.exp(-x))\n",
        "        return 0\n",
        "\n",
        "    def tanh(self, x, grad=False):\n",
        "        if grad:\n",
        "            return 1.0 - np.tanh(x)**2\n",
        "            pass\n",
        "        return np.tanh(x)\n",
        "        pass\n",
        "        return 0\n",
        "\n",
        "    def activation(self, x, grad=False):\n",
        "        \n",
        "        if self.activation_str == \"relu\":\n",
        "            return self.relu(x,grad)\n",
        "        \n",
        "        elif self.activation_str == \"sigmoid\":\n",
        "            return self.sigmoid(x, grad)\n",
        "        \n",
        "        elif self.activation_str == \"tanh\":\n",
        "            return self.tanh(x,grad)\n",
        "        \n",
        "        else:\n",
        "            raise Exception(\"invalid\")\n",
        "        return 0\n",
        "\n",
        "    def softmax(self, x):\n",
        "        \n",
        "        z = x - np.max(x, axis=-1, keepdims=True)\n",
        "        numerator = np.exp(z)\n",
        "        denominator = np.sum(numerator, axis=-1, keepdims=True)\n",
        "        softmax = numerator / denominator\n",
        "        return softmax\n",
        " \n",
        "\n",
        "    def forward(self, x):\n",
        "        cache = {\"Z0\": x}\n",
        "        \n",
        "        Z = x.T\n",
        "        cache[f\"Z{str(0)}\"] = Z\n",
        "        for layer_n in range(1, self.n_hidden + 1):\n",
        "            W = self.weights[\"W\" + str(layer_n)]\n",
        "            b = self.weights[\"b\" + str(layer_n)]\n",
        "            # print('Shape W' + str(np.transpose(W).shape))\n",
        "            # print('Shape b' + str(np.transpose(b).shape))\n",
        "\n",
        "            A = np.dot(np.transpose(W), Z) + np.transpose(b)\n",
        "            Z = self.activation(A)\n",
        "            cache[f\"A{str(layer_n)}\"] = A\n",
        "            cache[f\"Z{str(layer_n)}\"] = Z\n",
        "\n",
        "        #Apply softmax\n",
        "        W = self.weights[\"W\" + str(self.n_hidden + 1)]\n",
        "        b = self.weights[\"b\" + str(self.n_hidden + 1)]\n",
        "        # print('Shape W' + str(np.transpose(W).shape))\n",
        "        # print('Shape b' + str(np.transpose(b).shape))\n",
        "\n",
        "        A = np.dot(np.transpose(W), Z) + np.transpose(b)\n",
        "        Z = self.softmax(A)\n",
        "        cache[f\"A{str(self.n_hidden + 1)}\"] = A\n",
        "        cache[f\"Z{str(self.n_hidden + 1)}\"] = Z\n",
        "         \n",
        "        return cache\n",
        "\n",
        "    def backward(self, cache, labels):\n",
        "        output = cache[f\"Z{self.n_hidden + 1}\"]\n",
        "        grads = {}\n",
        "        n = len(labels)\n",
        "        dA = output - labels.T\n",
        "        prev_activ = cache[f\"Z\" + str(self.n_hidden)]\n",
        "        dW = (1. / n) * np.dot(dA, prev_activ.T)\n",
        "        db = (1. / n) * np.sum(dA, axis=1, keepdims=True)\n",
        "\n",
        "        dZ = np.dot(self.weights[\"W\" + str(self.n_hidden + 1)], dA)\n",
        "        grads[\"dW\" + str(self.n_hidden + 1)] = dW\n",
        "        grads[\"db\" + str(self.n_hidden + 1)] = db\n",
        "\n",
        "        for layer_n in range(self.n_hidden, 0, -1):\n",
        "            grad_prev_activ = self.activation(cache[\"A\" + str(layer_n)],True)\n",
        "            dA = dZ * grad_prev_activ\n",
        "            prev_activ_v = cache[\"Z\" + str(layer_n - 1)]\n",
        "            dW = 1./n * np.dot(dA, prev_activ_v.T)\n",
        "            db = 1./n * np.sum(dA, axis=1, keepdims=True)\n",
        "            if layer_n >= 1:\n",
        "                W_prev_layer = self.weights[\"W\" + str(layer_n)]\n",
        "                dZ = np.dot(W_prev_layer, dA)\n",
        "            grads[\"dW\" + str(layer_n)] = dW\n",
        "            grads[\"db\" + str(layer_n)] = db# WRITE CODE HERE\n",
        "        return grads\n",
        "\n",
        "\n",
        "    def update(self, grads):\n",
        "        for layer in range(1, self.n_hidden + 2):\n",
        "            W = self.weights[\"W\" + str(layer)]\n",
        "            b = self.weights[\"b\" + str(layer)]\n",
        "\n",
        "            W = W - self.lr * grads[\"dW\" + str(layer)].T\n",
        "            b = b - self.lr * grads[\"db\" + str(layer)].T\n",
        "\n",
        "            self.weights.update({\"W\" + str(layer): W, \"b\" + str(layer): b})\n",
        "\n",
        "    # def one_hot(self, y, n_classes=None):\n",
        "    #     n_classes = n_classes or self.n_classes\n",
        "    #     return np.eye(n_classes)[y]\n",
        "\n",
        "    def loss(self, prediction, labels):\n",
        "        prediction[np.where(prediction < self.epsilon)] = self.epsilon\n",
        "        prediction[np.where(prediction > 1 - self.epsilon)] = 1 - self.epsilon\n",
        "        prediction = np.multiply(prediction, labels)\n",
        "        precision = np.max(prediction, axis=1)\n",
        "        log_precision = np.log(precision, out=np.zeros_like(precision), where=(precision != 0))\n",
        "        log_err = np.multiply(log_precision, -1)\n",
        "        return np.mean(log_err)\n",
        "        \n",
        "        return 0\n",
        "        \n",
        "\n",
        "    def compute_loss_and_accuracy(self, X, y):\n",
        "        one_y = y\n",
        "        y = np.argmax(y, axis=1)  # Change y to integers\n",
        "        cache = self.forward(X)\n",
        "        predictions = np.argmax(cache[f\"Z{self.n_hidden + 1}\"], axis=1)\n",
        "        accuracy = np.mean(y == predictions)\n",
        "        loss = self.loss(cache[f\"Z{self.n_hidden + 1}\"], one_y)\n",
        "        return loss, accuracy, predictions\n",
        "\n",
        "    def train_loop(self, n_epochs):\n",
        "        X_train, y_train = self.train\n",
        "        y_onehot = y_train\n",
        "        dims = [X_train.shape[1], y_onehot.shape[1]]\n",
        "        self.initialize_weights(dims)\n",
        "\n",
        "        n_batches = int(np.ceil(X_train.shape[0] / self.batch_size))\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            for batch in range(n_batches):\n",
        "                minibatchX = X_train[self.batch_size * batch:self.batch_size * (batch + 1), :]\n",
        "                minibatchY = y_onehot[self.batch_size * batch:self.batch_size * (batch + 1), :]\n",
        "                #Forward\n",
        "                cache = self.forward(minibatchX)\n",
        "                grads = self.backward(cache, minibatchY)\n",
        "                self.update(grads)\n",
        "                \n",
        "\n",
        "            X_train, y_train = self.train\n",
        "            train_loss, train_accuracy, _ = self.compute_loss_and_accuracy(X_train, y_train)\n",
        "            X_valid, y_valid = self.valid\n",
        "            valid_loss, valid_accuracy, _ = self.compute_loss_and_accuracy(X_valid, y_valid)\n",
        "\n",
        "            self.train_logs['train_accuracy'].append(train_accuracy)\n",
        "            self.train_logs['validation_accuracy'].append(valid_accuracy)\n",
        "            self.train_logs['train_loss'].append(train_loss)\n",
        "            self.train_logs['validation_loss'].append(valid_loss)\n",
        "\n",
        "        return self.train_logs\n",
        "\n",
        "    def evaluate(self):\n",
        "        X_test, y_test = self.test\n",
        "        test_loss, test_accuracy, _ = self.compute_loss_and_accuracy(X_test, y_test)\n",
        "        return test_loss, test_accuracy\n",
        "\n",
        "nn = NN(hidden_dims=(600,700), data=data)\n",
        "print(nn.train_loop(10))\n",
        "nn = NN(hidden_dims=(400,500))\n",
        "nn.train_loop(10)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-16bd45845045>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_and_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-16bd45845045>\u001b[0m in \u001b[0;36mload_mnist\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mnist.pkl.gz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdata_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mnist.pkl.gz'"
          ]
        }
      ]
    }
  ]
}