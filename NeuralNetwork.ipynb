{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetwork",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samibahig/IFT6135/blob/main/NeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP6QdBstSXpu"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import gzip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rviv9pu8SYmM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "ca6c7240-55e4-418f-a68e-c08df0c7a3cf"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import gzip\n",
        "import tqdm\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "\n",
        "def one_hot(y, n_classes=10):\n",
        "    return np.eye(n_classes)[y]\n",
        "\n",
        "\n",
        "def load_mnist():\n",
        "    data_file = gzip.open(\"mnist.pkl.gz\", \"rb\")\n",
        "    train_data, val_data, test_data = pickle.load(data_file, encoding=\"latin1\")\n",
        "    data_file.close()\n",
        "\n",
        "    train_inputs = [np.reshape(x, (784, 1)) for x in train_data[0]]\n",
        "    train_results = [one_hot(y, 10) for y in train_data[1]]\n",
        "    train_data = np.array(train_inputs).reshape(-1, 784), np.array(train_results).reshape(-1, 10)\n",
        "\n",
        "    val_inputs = [np.reshape(x, (784, 1)) for x in val_data[0]]\n",
        "    val_results = [one_hot(y, 10) for y in val_data[1]]\n",
        "    val_data = np.array(val_inputs).reshape(-1, 784), np.array(val_results).reshape(-1, 10)\n",
        "\n",
        "    test_inputs = [np.reshape(x, (784, 1)) for x in test_data[0]]\n",
        "    test_data = list(zip(test_inputs, test_data[1]))\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "\n",
        "#train_data_, val_data_, test_data_ = load_mnist()\n",
        "\n",
        "class NN(object):\n",
        "    def __init__(self,\n",
        "                 hidden_dims=(784, 256),\n",
        "                 epsilon=1e-6,\n",
        "                 lr=7e-4,\n",
        "                 batch_size=64,\n",
        "                 seed=None,\n",
        "                 activation=\"relu\",\n",
        "                 data=None\n",
        "                 ):\n",
        "\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.n_hidden = len(hidden_dims)\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.init_method = 'Glorot'\n",
        "        self.seed = seed\n",
        "        self.activation_str = activation\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        self.train_logs = {'train_accuracy': [], 'validation_accuracy': [], 'train_loss': [], 'validation_loss': []}\n",
        "\n",
        "        if data is None:\n",
        "            # for testing, do NOT remove or modify\n",
        "            self.train, self.valid, self.test = (\n",
        "                (np.random.rand(400, 784), one_hot(np.random.randint(0, 10, 400))),\n",
        "                (np.random.rand(400, 784), one_hot(np.random.randint(0, 10, 400))),\n",
        "                (np.random.rand(400, 784), one_hot(np.random.randint(0, 10, 400)))\n",
        "            )\n",
        "        else:\n",
        "            self.train, self.valid, self.test = data\n",
        "\n",
        "    def initialize_weights(self, dims):\n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed)\n",
        "\n",
        "        self.weights = {}\n",
        "        # self.weights is a dictionnary with keys W1, b1, W2, b2, ..., Wm, Bm where m - 1 is the number of hidden layers\n",
        "        all_dims = [dims[0]] + list(self.hidden_dims) + [dims[1]]\n",
        "        for layer_n in range(1, self.n_hidden + 2):\n",
        "            d = np.sqrt(1 / all_dims[layer_n - 1])\n",
        "            W = np.random.uniform(-d, d, (all_dims[layer_n - 1], all_dims[layer_n]))\n",
        "            self.weights[f\"W{layer_n}\"] = W\n",
        "            self.weights[f\"b{layer_n}\"] = np.zeros((1, all_dims[layer_n]))\n",
        "\n",
        "    def relu(self, x, grad=False):\n",
        "        if grad:\n",
        "            return x > 0\n",
        "            pass\n",
        "        return np.maximum(x, 0, x)\n",
        "        pass\n",
        "        return 0\n",
        "\n",
        "    def sigmoid(self, x, grad=False):\n",
        "        if grad:\n",
        "            s = 1 / (1 + np.exp(-x))\n",
        "            return s * (s - 1)\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "        return 0\n",
        "\n",
        "    def tanh(self, x, grad=False):\n",
        "        if grad:\n",
        "            return 1.0 - np.tanh(x) ** 2\n",
        "        return np.tanh(x)\n",
        "        pass\n",
        "        return 0\n",
        "\n",
        "    def activation(self, x, grad=False):\n",
        "        rst = 0\n",
        "        if self.activation_str == \"relu\":\n",
        "            rst=self.relu(x, grad)\n",
        "        elif self.activation_str == \"sigmoid\":\n",
        "            rst=self.sigmoid(x, grad)\n",
        "        elif self.activation_str == \"tanh\":\n",
        "            rst=self.tanh(x, grad)\n",
        "        else:\n",
        "            raise Exception(\"invalid\")\n",
        "        return rst\n",
        "\n",
        "    def softmax(self, x):\n",
        "        # Remember that softmax(x-C) = softmax(x) when C is a constant.\n",
        "        exps = np.exp(x)\n",
        "        return exps / np.sum(exps, axis=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        cache = {\"Z0\": x}\n",
        "        # cache is a dictionnary with keys Z0, A0, ..., Zm, Am where m - 1 is the number of hidden layers\n",
        "        # Ai corresponds to the preactivation at layer i, Zi corresponds to the activation at layer i\n",
        "        # WRITE CODE HERE\n",
        "        Z = x.T\n",
        "        cache[f\"Z{str(0)}\"] = Z\n",
        "        for layer_n in range(1, self.n_hidden + 1):\n",
        "            W = self.weights[\"W\" + str(layer_n)]\n",
        "            b = self.weights[\"b\" + str(layer_n)]\n",
        "            # print('Shape W' + str(np.transpose(W).shape))\n",
        "            # print('Shape b' + str(np.transpose(b).shape))\n",
        "\n",
        "            A = np.dot(np.transpose(W), Z) + np.transpose(b)\n",
        "            Z = self.activation(A)\n",
        "            cache[f\"A{str(layer_n)}\"] = A\n",
        "            cache[f\"Z{str(layer_n)}\"] = Z\n",
        "\n",
        "        #Apply softmax\n",
        "        W = self.weights[\"W\" + str(self.n_hidden + 1)]\n",
        "        b = self.weights[\"b\" + str(self.n_hidden + 1)]\n",
        "        # print('Shape W' + str(np.transpose(W).shape))\n",
        "        # print('Shape b' + str(np.transpose(b).shape))\n",
        "\n",
        "        A = np.dot(np.transpose(W), Z) + np.transpose(b)\n",
        "        Z = self.softmax(A)\n",
        "        cache[f\"A{str(self.n_hidden + 1)}\"] = A\n",
        "        cache[f\"Z{str(self.n_hidden + 1)}\"] = Z\n",
        "        \n",
        "        return cache\n",
        "\n",
        "    def backward(self, cache, labels):\n",
        "        # print(\"cache \" + str(cache.keys()))\n",
        "        output = cache[f\"Z{self.n_hidden + 1}\"]\n",
        "        n = len(labels)\n",
        "        grads = {}\n",
        "        dA = output - labels.T\n",
        "        prev_activ = cache[f\"Z\" + str(self.n_hidden)]\n",
        "        dW = (1. / n) * np.dot(dA, prev_activ.T)\n",
        "        db = (1. / n) * np.sum(dA, axis=1, keepdims=True)\n",
        "\n",
        "        dZ = np.dot(self.weights[\"W\" + str(self.n_hidden + 1)], dA)\n",
        "        grads[\"dW\" + str(self.n_hidden + 1)] = dW\n",
        "        grads[\"db\" + str(self.n_hidden + 1)] = db\n",
        "\n",
        "        for layer_n in range(self.n_hidden, 0, -1):\n",
        "            grad_prev_activ = self.activation(cache[\"A\" + str(layer_n)],True)\n",
        "            dA = dZ * grad_prev_activ\n",
        "            prev_activ_v = cache[\"Z\" + str(layer_n - 1)]\n",
        "            dW = 1./n * np.dot(dA, prev_activ_v.T)\n",
        "            db = 1./n * np.sum(dA, axis=1, keepdims=True)\n",
        "            if layer_n >= 1:\n",
        "                W_prev_layer = self.weights[\"W\" + str(layer_n)]\n",
        "                dZ = np.dot(W_prev_layer, dA)\n",
        "            grads[\"dW\" + str(layer_n)] = dW\n",
        "            grads[\"db\" + str(layer_n)] = db\n",
        "        return grads\n",
        "\n",
        "    def update(self, grads):\n",
        "        for layer in range(1, self.n_hidden + 2):\n",
        "            # WRITE CODE HERE\n",
        "            W = self.weights[\"W\" + str(layer)]\n",
        "            b = self.weights[\"b\" + str(layer)]\n",
        "\n",
        "            W = W - self.lr * grads[\"dW\" + str(layer)].T\n",
        "            b = b - self.lr * grads[\"db\" + str(layer)].T\n",
        "\n",
        "            self.weights.update({\"W\" + str(layer): W, \"b\" + str(layer): b})\n",
        "\n",
        "    # def one_hot(self, y, n_classes=None):\n",
        "    #     n_classes = n_classes or self.n_classes\n",
        "    #     return np.eye(n_classes)[y]\n",
        "\n",
        "    def loss(self, prediction, labels):\n",
        "        prediction = np.multiply(prediction, labels)\n",
        "        precision = np.max(prediction, axis=1)\n",
        "        log_precision = np.log(precision, out=np.zeros_like(precision), where=(precision != 0))\n",
        "        log_err = np.multiply(log_precision, -1)\n",
        "        err = np.mean(log_err)\n",
        "        return err\n",
        "\n",
        "    def compute_loss_and_accuracy(self, X, y):\n",
        "        one_y = y\n",
        "        y = np.argmax(y, axis=1)  # Change y to integers\n",
        "        cache = self.forward(X)\n",
        "        predictions = np.argmax(cache[f\"Z{self.n_hidden + 1}\"], axis=0)\n",
        "        accuracy = np.mean(y == predictions)\n",
        "        loss = self.loss(cache[f\"Z{self.n_hidden + 1}\"], one_y.T)\n",
        "        return loss, accuracy, predictions\n",
        "\n",
        "    def train_loop(self, n_epochs):\n",
        "        X_train, y_train = self.train\n",
        "        y_onehot = y_train\n",
        "        dims = [X_train.shape[1], y_onehot.shape[1]]\n",
        "        self.initialize_weights(dims)\n",
        "\n",
        "        n_batches = int(np.ceil(X_train.shape[0] / self.batch_size))\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            for batch in range(n_batches):\n",
        "                minibatchX = X_train[self.batch_size * batch:self.batch_size * (batch + 1), :]\n",
        "                minibatchY = y_onehot[self.batch_size * batch:self.batch_size * (batch + 1), :]\n",
        "                # WRITE CODE HERE\n",
        "                # Forward\n",
        "                cache = self.forward(minibatchX)\n",
        "                # Backward\n",
        "                grads = self.backward(cache, minibatchY)\n",
        "                # Update\n",
        "                self.update(grads)\n",
        "\n",
        "            X_train, y_train = self.train\n",
        "            train_loss, train_accuracy, _ = self.compute_loss_and_accuracy(X_train, y_train)\n",
        "            X_valid, y_valid = self.valid\n",
        "            valid_loss, valid_accuracy, _ = self.compute_loss_and_accuracy(X_valid, y_valid)\n",
        "\n",
        "            self.train_logs['train_accuracy'].append(train_accuracy)\n",
        "            self.train_logs['validation_accuracy'].append(valid_accuracy)\n",
        "            self.train_logs['train_loss'].append(train_loss)\n",
        "            self.train_logs['validation_loss'].append(valid_loss)\n",
        "\n",
        "        return self.train_logs\n",
        "\n",
        "    def evaluate(self):\n",
        "        X_test, y_test = self.test\n",
        "        test_loss, test_accuracy, _ = self.compute_loss_and_accuracy(X_test, y_test)\n",
        "        return test_loss, test_accuracy\n",
        "\n",
        "\n",
        "data = load_mnist()\n",
        "\n",
        "nn = NN(hidden_dims=(600,700), data=data)\n",
        "print(nn.train_loop(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e8e402361d3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-e8e402361d3d>\u001b[0m in \u001b[0;36mload_mnist\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mnist.pkl.gz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdata_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mnist.pkl.gz'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSvgxYkRDAha",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "934c83cd-ba70-428f-d397-14118963e613"
      },
      "source": [
        "nn = NN(hidden_dims=(400,500))\n",
        "nn.train_loop(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape W(400, 784)\n",
            "Shape b(400, 1)\n",
            "----- Z1\n",
            "Shape W(500, 400)\n",
            "Shape b(500, 1)\n",
            "----- Z2\n",
            "Shape W(10, 500)\n",
            "Shape b(10, 1)\n",
            "----- Z3\n",
            "cache dict_keys(['Z0', 'A1', 'Z1', 'A2', 'Z2', 'A3', 'Z3'])\n",
            "dA.shape (500, 64)\n",
            "t (500, 400)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-bb14e1d23e80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-67343b3210c7>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatchX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;31m#Backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminibatchY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m                 \u001b[0;31m#Update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-67343b3210c7>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, cache, labels)\u001b[0m\n\u001b[1;32m    152\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m           \u001b[0mdZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m           \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m           \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"db\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (500,400) and (500,64) not aligned: 400 (dim 1) != 500 (dim 0)"
          ]
        }
      ]
    }
  ]
}